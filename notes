 - theoretische argumente für optimierung über regression
 - motivation für imperfekte invertierung?

thoughts on the viability of foe priors for net inversion:

early layers with high dim get beaten by img prior + conv, because the latter have way fewer degrees of freedom
late layers might actually contribute something but by then the number of channels increases further so full priors are not viable,
channelwise priors might, especially if extracted features become more independent. overall though, as feature maps get smaller,
the stationarity has less of an effect. this may serve to explain why feature map priors add nothing do improve image level reconstructions.


ich kann das berechene des FoE priors nicht generell als eine convolution über die feature map formulieren, weil ich jedes patch noch normen muss.
wenn ich aber einen fixen channel mean und channel standard deviation nehme, geht das möglicherweise, weil ich dann direkt die feature map normen kann.
dann ist die whitening+mixing operation tatsächlich eine convolution.
allerdings lohnt sich das nur bei inferenzzeit, so wie ich die training samples jetzt preprocesse. könnte mit der erkenntnis aber auch nochmal drüber nachdenken,
auf ganzen bildern zu trainieren? besser aber nicht, ich will im training set zumindest das normen und whitening richtig berechnen können, da macht ein kleinerer
Datensatz von patches schon sinn.


- bei abziehen des training set means geht bei den patches nicht wirklich ein freiheitsgrad verloren. soll ich also darauf verzichten, eine dimension abzuzuiehen?
ich verlasse mich ja sonst eigentlich drauf, dass die annahme gilt.


- test trained 12x12 image prior
- implement remaining loss metrics
- test channelwise student prior
- similarity measures for feature maps...

- full prior conv features

- train simple dobro model for reference
- use normal sgd for opt inv
- consider refactoring naming, to differentiate whitening methods

One of each conv2/lin to conv1/lin:
- FoE Full prior (done)
- FoE Channelwise Prior (done-ish)
- FoE Separable Prior ----
- FoE slim Full Prior (done)
- FoE Channelwise Prior + slim Full Prior ()
- Mahendran Vedaldi (done tv prior)
- Dobro Model ----
- pure MSE (done)

avg over 3 runs

separable prior trains useful image filters even on all channel zca whitened data
separable prior training still firmly in quadratic complexity wrt the components
Dosovitskiy Brox don't actually invert to real image dimensions

results complitation
mache ich schriftlich

lbgfs keys:
epsilon=1e-5,
num_random_targets=0,
maxiter=150,
verbose=False

saliency map keys
max_iter=2000,
num_random_targets=0,
fast=True,
theta=0.1,
max_perturbations_per_pixel=7

gradsign keys
epsilons=1000

itgradsign keys
epsilons=100,
steps=10

deepfool
steps=100,
subsample=10

idx, name, label-id

53 light switch, 844
76 barber shop, 424
81 alp, 970
99 strawberry, 949
106 windsor tie, 906
108 cello, 486
129 hotdog, 934
153 iguana, 39
157 red fox, 277
160 maze, 646

lbfgs + 12x12 img prior
Loss    33.0931 for image val53_t844_f39
Loss     0.6284 for image val53_t844_f844
Loss     3.6122 for image val53_t844_f970
Loss     2.5478 for image val76_t860_f424
Loss     2.8565 for image val81_t970_f970
Loss    11.1615 for image val99_t949_f934
Loss     1.4844 for image val99_t949_f949
Loss     3.8146 for image val99_t949_f970
Loss     2.6395 for image val106_t824_f906
Loss     4.5298 for image val108_t889_f486

originals + 12x12 img prior
Loss     0.6284 for image val53
Loss     2.5454 for image val76
Loss     2.8565 for image val81
Loss     1.4844 for image val99
Loss     2.6216 for image val106
Loss     4.5153 for image val108
Loss     3.3293 for image val129
Loss     6.4354 for image val153
Loss     0.5108 for image val157
Loss     4.6791 for image val160

mean difference: -0.00403496
number of images with higher ad-ex loss 147
number of images with lower or equal ad-ex loss 51

done 1: remake 200 advex, store as floats this time


coarse experiemt: lr = 1.0 with ADAM
log points after n iterations                       [  1, 2, 3, 4, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 60, 70, 80, 90, 100]

count of changes in images                          [  2, 2, 2, 6, 1,  9, 11, 10,  4, 10, 11,  9, 13,  8, 18, 21, 21, 14,  15]
count of first changes in images                    [  2, 2, 1, 6, 1,  8,  8,  7,  3,  6,  9,  6,  9,  6,  6,  9,  8,  7,   5]

count of changes in adv ex                          [161, 4, 3, 5, 0, 15, 12,  7, 13, 12, 12,  9, 13,  6, 18, 22, 21, 12,  21]
count of first changes in adv ex                    [161, 3, 1, 3, 0,  4,  5,  0,  3,  4,  0,  2,  1,  0,  4,  4,  0,  0,   0]
count of first changes to original label in adv ex  [161, 3, 1, 1, 0,  4,  2,  0,  2,  2,  0,  0,  1,  0,  1,  0,  0,  0,   0]
count of preserved images at each time step         [198, 196, 194, 193, 187, 186, 178, 171, 165, 163, 158, 149, 144, 135, 128, 123, 115, 106, 99, 93]
count of restored advex at each time step           [0, 161, 163, 162, 161, 161, 155, 155, 150, 147, 146, 138, 134, 128, 124, 121, 112, 102, 97, 88]

fine experiment lr = 0.1 with ADAM
log points after n iterations                       [  1,  2,  3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]

count of changes in images                          [  0,  0,  1, 0, 0, 0, 0, 0, 0,  1,  0,  0,  0,  0,  0,  0,  0,  2,  0]
count of first changes in images                    [  0,  0,  1, 0, 0, 0, 0, 0, 0,  1,  0,  0,  0,  0,  0,  0,  0,  2,  0]
count of changes in adv ex                          [115, 12, 10, 5, 2, 7, 1, 3, 3,  0,  1,  0,  0,  1,  2,  0,  1,  2,  0]
count of first changes in adv ex                    [115, 12, 10, 4, 2, 6, 1, 3, 3,  0,  1,  0,  0,  1,  2,  0,  0,  2,  0]
count of first changes to original label in adv ex  [115, 12, 10, 4, 2, 6, 1, 3, 3,  0,  1,  0,  0,  1,  2,  0,  0,  2,  0]
count of preserved images at each time step         [198, 198, 198, 197, 197, 197, 197, 197, 197, 197, 196, 196, 196, 196, 196, 196, 196, 196, 194, 194]
count of restored advex at each time step           [0, 115, 127, 137, 140, 142, 147, 148, 151, 154, 154, 155, 155, 155, 156, 158, 158, 157, 159, 159]

dropout prior stability experiment:
fine experiment lr = 0.1 with ADAM
log points after n iterations                       [  1,  2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]

count of changes in images                          [  0,  1, 0, 0, 0, 0, 0, 0, 0,  1,  0,  0,  0,  0,  0,  0,  1,  1,  0]
count of first changes in images                    [  0,  1, 0, 0, 0, 0, 0, 0, 0,  1,  0,  0,  0,  0,  0,  0,  1,  1,  0]
count of changes in adv ex                          [120, 12, 2, 7, 3, 4, 4, 3, 1,  3,  1,  1,  2,  1,  1,  3,  0,  0,  0]
count of first changes in adv ex                    [120, 12, 2, 7, 3, 4, 4, 3, 1,  3,  1,  1,  2,  0,  1,  3,  0,  0,  0]
count of first changes to original label in adv ex  [120, 12, 2, 7, 3, 4, 4, 3, 1,  3,  1,  1,  2,  0,  1,  3,  0,  0,  0]
count of preserved images at each time step         [198, 198, 197, 197, 197, 197, 197, 197, 197, 197, 196, 196, 196, 196, 196, 196, 196, 195, 194, 194]
count of restored advex at each time step           [  0, 120, 132, 134, 141, 144, 148, 152, 155, 156, 159, 160, 161, 163, 162, 163, 166, 166, 166, 166]


count of changes in images                          [1, 1, 3, 0, 1, 0, 3, 3, 2, 0, 3, 2, 3, 2, 0, 3, 0, 2, 4]
count of changes in adv ex                          [657, 67, 47, 32, 25, 15, 21, 13, 11, 9, 6, 4, 8, 10, 5, 6, 3, 3, 3]
count of first changes in images                    [1, 1, 3, 0, 1, 0, 3, 3, 2, 0, 3, 2, 3, 2, 0, 3, 0, 2, 3]
count of first changes in adv ex                    [657, 67, 47, 32, 24, 12, 20, 12, 11, 8, 5, 3, 7, 10, 5, 4, 3, 1, 3]
count of first changes to original label in adv ex  [657, 67, 47, 32, 24, 12, 20, 12, 11, 8, 5, 3, 6, 9, 5, 4, 3, 1, 3]
count of preserved images at each time step         [1069, 1068, 1067, 1064, 1064, 1063, 1063, 1060, 1057, 1055, 1055, 1052, 1050, 1047, 1045, 1045, 1042, 1042, 1040, 1037]
count of restored advex at each time step           [0, 657, 724, 771, 803, 826, 835, 854, 865, 876, 883, 887, 889, 894, 903, 908, 910, 913, 912, 915]


ensemble strategie:
Ist es möglich, ein adversarial example zu kostruieren, das in allen layern nah an der geschätzten datendistribution liegt?
Whitebox attacken auf metzen und li suggerieren ja, wobei das classifier waren. mit goodfellows argument, sollten auch prior betroffen sein. ensembles scheinen also schwer umzusetzen.

2x2 mean filter
number of samples contained: 198
labels retained after smoothing image 150
labels retained after adversarial attack 2
labels restored after smoothing advex 136
image and advex smoothed to same label 184

3x3 mean filter
number of samples contained: 198
labels retained after smoothing image 120
labels retained after adversarial attack 2
labels restored after smoothing advex 114
labels retained after smoothing image [1069 1022  974  929  877  815  753  698  633  606  621]
labels restored after smoothing advex [  0 728 751 734 717 690 645 606 567 541 560]


number of samples contained: 198
labels retained after smoothing image 103
labels retained after adversarial attack 2
labels restored after smoothing advex 96

eval no-drop whitebox results
# attacks failed within allowed steps 32
# errors during attack 7
# successful attacks 1030
maxmin of adaptive noise 581.254272461 0.0417845547199
meanmedian diff 0.0181729805846 -1.40358924866
meanmedian frac 1.06874516405 0.953514860834
# adative noise > oblibious noise 241
# adative noise < oblibious noise 789

eval do-drop on adaptive examples

eval do-drop on oblivious examples
count of preserved images at each time step
[1069, 1068, 1067, 1064, 1064,
1063, 1063, 1060, 1057, 1055,
1055, 1052, 1050, 1047, 1045,
1045, 1042, 1042, 1040, 1037]
count of restored advex at each time step
[0, 657, 724, 771, 803, 826,
835, 854, 865, 876, 883,
887, 889, 894, 903, 908,
910, 913, 912, 915]

eval no-drop on oblivious examples
count of preserved images at each time step
[1069, 1068, 1067, 1066, 1064,
1064, 1063, 1060, 1056, 1054,
1051, 1050, 1049, 1048, 1047,
1043, 1042, 1039, 1038, 1036,
1035, 1034, 1034, 1031, 1031,
1030, 1030, 1030, 1028, 1027]
count of restored advex at each time step
[0, 612, 712, 763, 795,
809, 826, 839, 852, 855,
864, 872, 882, 883, 885,
890, 896, 899, 900, 903,
904, 906, 908, 910, 910,
909, 907, 906, 905, 904]


eval full
count of preserved images at each time step
[1069, 1068, 1067, 1067, 1065,
1064, 1062, 1061, 1059, 1055,
1051, 1050, 1050, 1048, 1047,
1046, 1044, 1044, 1039, 1037]
count of restored advex at each time step
[0, 613, 705, 761, 797,
817, 831, 841, 851, 868,
874, 878, 887, 892, 892,
892, 892, 891, 898, 902]


dropout prior dodrop oblivious
count of changes in images                          [1, 1, 1, 2, 0, 1, 3, 4, 2, 3, 1, 1, 1, 1, 4, 1, 3, 1, 2, 2, 1, 0, 3, 0, 1, 0, 0, 2, 1]
count of changes in adv ex                          [612, 100, 53, 32, 14, 21, 19, 15, 7, 13, 8, 11, 3, 2, 6, 6, 5, 5, 3, 1, 2, 3, 6, 6, 1, 2, 3, 6, 5]
count of first changes in images                    [1, 1, 1, 2, 0, 1, 3, 4, 2, 3, 1, 1, 1, 1, 4, 1, 3, 1, 2, 1, 1, 0, 3, 0, 1, 0, 0, 2, 1]
count of first changes in adv ex                    [612, 100, 52, 32, 14, 19, 16, 14, 5, 11, 7, 11, 2, 2, 6, 6, 3, 2, 3, 1, 2, 3, 4, 3, 0, 0, 1, 3, 2]
count of first changes to original label in adv ex  [612, 100, 52, 32, 14, 19, 16, 14, 5, 11, 7, 10, 2, 2, 5, 6, 3, 2, 3, 1, 2, 2, 4, 3, 0, 0, 1, 2, 2]
count of preserved images at each time step         [1069, 1068, 1067, 1066, 1064, 1064, 1063, 1060, 1056, 1054, 1051, 1050, 1049, 1048, 1047, 1043,
1042, 1039, 1038, 1036, 1035, 1034, 1034, 1031, 1031, 1030, 1030, 1030, 1028, 1027]
count of restored advex at each time step           [0, 612, 712, 763, 795, 809, 826, 839, 852, 855, 864, 872, 882, 883, 885, 890, 896, 899, 900, 903, 904, 906, 908, 910, 910, 909,
907, 906, 905, 904]


fullpriors adaptive:
count of preserved images at each time step     [1056, 1044, 1036, 1029, 1023, 1021, 1015, 1007, 1001, 996]
count of restored advex at each time step       [1042, 1027, 1013,  991,   65,  102,  163,  247,  326, 420]

43659085274


mean filtering results on deepfool with padding error, resulting in single pixel translation down and to the right, with Reflect padding on the top and left borders
labels retained after smoothing image [1069 1031  995  958  928  893  862  839  812  793  791]
labels restored after smoothing advex [997 993 955 917 877 837 808 769 751 725 728]
image and advex smoothed to same label [ 997 1012 1011 1003 1007 1006  993  981  991  980  988]

results after fixing this error
labels retained after smoothing image [1069 1027  991  964  934  895  866  821  802  780  776]
labels restored after smoothing advex [  0 766 784 782 779 757 738 729 708 697 694]
image and advex smoothed to same label [  0 808 859 881 905 922 935 963 966 978 974]


performance on adaptive adversarials aimed at a weight of 0.2
labels retained after smoothing image [1068 1026  990  963  933  894  865  820  801  779  775]
labels restored after smoothing advex [609 546  60 661 685 691 693 680 675 669 674]
image and advex smoothed to same label [609 524   0 759 811 858 883 916 926 946 950]

max approx with softmax?
cross label softmax as factors

sum_k ( sofmax_k(V_kl) * V_kl ) is a lower bound on max_k(V_kl) which is always better than mean_k(V_kl), as sofmax_k(V_kl) > 1/K for k > 1/K

what to do with the other labels?

CNN-INV:
create stacked and baked results for the 10 example images
- store with cnn inv logs / imgs

note on image scaling: decided to only bound plots (floor/ceil on values outside) then optimized error is an upper bound on reconstruction mse.

work order opt inv
- find best dist and preprocessing under conv2/lin to img
- try 512 and 1024 components under conv2/lin to img
- try one 10x10 prior under conv2/lin to img
set prior.

for mv change points find optimal prior weight, try jitter for high change point
run test series with fox

weighting notes:
alexnet:
c1l: 1e-6 j0
c2l: 3e-3 j2
c3l: 1e-3 j4
c4l: 1e-4 j4
c5l: 1e-4 j4
f6l: 1e-4 j8
f7l: 1e-4 j8
f8l: 1e-4 j8
smx: 1e-4 j8


NOTE: Bad MSE partially due to jittering?

avg losses cxl layers
mv
vgg [ 0.18001309  0.44504634  0.5295279   0.64702719  0.72922671]
mse [  868.60314941  1391.61694336  1714.48474121  2197.02587891  3085.13134766]
nmse [ 0.06101058  0.09590761  0.1188912   0.15223107  0.2197659 ]
db
vgg [ 0.06099524  0.51719606  0.66417873  0.69428265  0.72960848]
mse [  188.44706726   457.82421875   845.15332031  1022.57891846  1419.92431641]
nmse [ 0.01347696  0.0343071   0.06194418  0.07467459  0.09801804]

foe
vgg [  0.00000951553557   0.215668529   0.371272117   0.439306349  0.599102378]
mse [  2.78555250  543.155457   923.226379   1334.41052   2317.35522]
nmse [  0.000229751269   0.0446296781   0.07.33917952   0.100476146  0.176144898]

vgg [  0.00000951553557   0.215668529   0.371272117   0.439306349  0.599102378   0.845661819   0.937098622   0.958883107]
mse [  2.78555250  543.155457   923.226379   1334.41052   2317.35522   4882.86328   5157.53613   5241.95361]
nmse [  0.000229751269   0.0446296781   0.07.33917952   0.100476146  0.176144898  0.409305185   0.437277168   0.440524399]

vgg [  9.51553557e-06   2.15668529e-01   3.71272117e-01   4.39306349e-01   5.99102378e-01   8.45661819e-01   9.37098622e-01   9.58883107e-01]
mse [  2.78555250e+00   5.43155457e+02   9.23226379e+02   1.33441052e+03   2.31735522e+03   4.88286328e+03   5.15753613e+03   5.24195361e+03]
nmse [  2.29751269e-04   4.46296781e-02   7.33917952e-02   1.00476146e-01  1.76144898e-01   4.09305185e-01   4.37277168e-01   4.40524399e-01]


Results - Checklist
CNN-Inversion, only alexnet
lin2lin module inversion 2to1, 3to2, 4to3, 5to4 with mse score for subset10 and feature map plots (merged and stacked)
img_rec inversion stacked from all conv layers, merged from lin layers (1, 4, 7, 8, 9) with plots, vgg mse and nmse scores for subset10

MV-Inversion, alexnet (& Vgg16?)
img_rec inversion from all layers with plots, vgg, mse and nmse scores for subset10 + fox

FoE-Inversion, alexnet (& Vgg16?)
img-rec inversion from all layers with plots, vgg, mse and nmse scores for subset10 + fox
lin2lin inversion 2to1, 3to2, 4to3, 5to4 with mse score for subset10 and feature map plots
fox inversion with pure mse

Featmap-Priors


Adversarials


TODO:
- turn off mean filter - test mean filter adaptive adversarials on classifier
- conv1/lin stability on fullprior adversarials

number of samples contained: 1073
labels retained after smoothing image [1073 1031  995  968  938  899  870  825  806  784  780]
labels restored after smoothing advex [  0  84 211 326 409 471 514 534 552 555 567]
image and advex smoothed to same label [  0 115 267 406 519 612 694 749 783 810 824]



mean filter 2x2, 20%
# correct classifiations with prior 991
# attacks failed within allowed steps 0
# errors during attack 0
# successful attacks 1069
means: oblivious 70.5566 adaptive 74.9628
maxmin of adaptive noise 597.035 0.179135
meanmedian diff 4.40629 3.02461
meanmedian frac 1.87941 1.0665
# adative noise > oblivious noise 684
# adative noise < oblivious noise 384

foe prior 4 steps, lr 0.6
# correct classifiations with prior 1034
# attacks failed within allowed steps 8
# errors during attack 5
# successful attacks 1056
means: oblivious 71.1892736641 adaptive 56.5042315162
maxmin of adaptive noise 501.887237549 0.0442688800395
meanmedian diff -14.6850421479 -10.8824253082
meanmedian frac 0.878276097582 0.770120355151
# adative noise > oblivious noise 57
# adative noise < oblivious noise 999


comp runs:
slim: 4 x
tv: 3   x
full: 4 or 5, probably 4 x
dual: 2 x
mse, momentum 3 adam 3
chan: 2 x



Short list of tasks
Part 1: Visualization
- Designed CNN-based benchmark based on Dosovitkiy and Brox
- Reproduced methods by Mahendran & Vedaldi 2015 + 2016
- Produced visualization using FoE image priors
- Evaluated results with MSE and VGG-score

Part 2: Feature map priors
- Tested regular and channelwise feature map priors with limited success
- Implemented depthwise separable conv priors
- Explored role of earlier network layers

Part 3:
- Tested weighted mean filter benchmark on oblivious and adaptive adversarials
- Tested regular FoE image prior on oblivious and adaptive adversarials
- Tested dropout FoE image prior on oblivious and adaptive adversarials
- Tested feature level FoE prior on oblivious and adaptive adversarials

oblivious       adaptive
71.18929291   56.50426865 norm at regularizer input
67.27068329   89.31594086 norm at regularizer output

oblivious       adaptive       image
655.76470947  651.06506348  648.51220703 norm at regularizer ouput wrt image at input
648.578125    648.2677002   648.51220703 norm at regularizer ouput wrt same at input



norm const basically absorbed in alpha?
alain bengio


Thesis TODO:

- rename sections
- consider splitting methods into preliminaries (before related work) and methods (after)
- add thing about interpretable prior somewhere
x cite i-RevNet jörn correctly
- find and fill [citation needed] notes

Intro:
x smoothe out text
x give example in first paragraph

Related Work:
x short intro paragraph
x smoothe out text

Methods:
x VGG score equation
x Tikz figures need fixing
x short section on whitening (ZCA only if referenced again in Separable prior)
x modular inverting could use a formula
x MV explain Jitter
x MRF ICA connection, why overcomplete, why whiten
x Advex finish intro and part on defenses
- fix mixing clique potentials formula by introducing weights
- unify feature map notation
o VGG only if any results with VGG are presented
o short section on Adam (move both further to the back, start with imagenet

Inversion Exps
- inversion to feature maps
- discussion
- vgg

Advex Exps
- smoothe

Conclusion
- merge featmap prior sections

Talk
- get bad fox taxidermy image, run on alexnet and vgg
