\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{placeins}

\title{MSc. Thesis - Deep Restoration\\Progress Updates}
\date{\today}
\author{Frederik Harder}

\begin{document}

\maketitle

\section*{sooner rather than later}

\begin{enumerate}
	\item skim simonyan papers, find the relevant ones
	\item do search on samplings from FoE models
	\item at some point look into hybrid mc sampling
\end{enumerate}

\section{August 15.}

\begin{enumerate}
	\item train lc - gc 1800f prior to convergence (amlab)
	\item train lc - rescale 1200f prior (laptop)
	\item train channel prior (laptop)
	\item write part on score matching, whitening
	\item note: conv1/lin is a weird layer. way more near 0 eigenvalues than e.g. lrn1 or conv2/lin (both have basically none), so way more redundancy in the filters, I suppose. Trained ICA priors do not seem to learn any useful filters.
	\item moving to lrn1 and conv2/lin for future tests instead.
\end{enumerate}



\section{August 14.}

\begin{enumerate}
	\item train conv1 lin prior with local full mean (center each sample) and global channel standard deviation normalization 
	\item filters don't look so good yet
	\item changed log-sum-exp expression in ica prior to a more special case, which should be numerically stable no matter what.
\end{enumerate}


\section{August 13.}

\begin{enumerate}
	\item pre-image init was bad. could perhaps still be tweaked (right now, adam needs to be restarted after the initial 500 steps, because its parameters get much too conservative)
	\item reproduced previous results on new pipeline (finally)
	\item feature maps still have way more eigenvalues close to zero.
\end{enumerate}


\section{August 12.}

\begin{enumerate}
	\item write out all the preprocessing
	\begin{enumerate}
		\item target and pre-image in range 0 - 255
		\item if no sdev normalization takes place, images are trained on rescaled patches in range 0-1 for later numerical stability of the ica prior.
		\item check that all the variables are loaded correctly
	\end{enumerate}
\end{enumerate}


\section{August 9.}

\begin{enumerate}
	\item integrate mean and sdev loading into the priors, adapt data-filenaming conventions
	\item create image dataset with gc,gc normalization
	\item enforce consistent feature map flattening - always flatten from [channels, height, width] - so each feature map lies on a continuous interval on the vector
	\item train gc,gc image ica prior
	\item train and test conv1lin feature map priors (1 ica, 1 cica)
\end{enumerate}

\section{August 8.}

\begin{enumerate}
	\item integrate mean and sdev processing funtion into the priors
	\item reproduce image priors (1 ica, 1 cica)
\end{enumerate}


\section*{August 7.}

\begin{enumerate}
	\item investigate assumptions behind current patch whitening procedure
	\begin{enumerate}
		\item typically one is suppose to center and rescale each feature before using pca \& whitening
		\item by stationarity assumption of stationarity, every feature should have the same mean. Subtracting the patch mean per channel (given equal variance) then takes care of the feature mean as well
		\item on natural images roughly equal variance across features (including different channels) is assumed.
		\item the stationarity assumption should hold for feature maps as well. the assumption of equal variance, however, is not as well-motivated.
		\item comparing patches from conv2/relu, conv1/lin\_cw and image layer, this can be observed to some degree, where standard deviations by channel range from roughly 20 to 30 in the image layer and from 2 to 70 and 12 to 300 in the other two layers. notably, within a channel in conv1/lin\_cw the values are again quite consistent, which supports the assumption of stationarity again.
	\end{enumerate}
	\item consider adding variance normalization, creating a new preprocessing pipeline
	\begin{enumerate}
		\item store truly raw (non-centered) patches in raw\_mat
		\item center all features by respective channel mean (global mean seems ill-advised even for priors with no channel independence assumptions), store 
		channel means for retrieval by prior
		\item then divide features by channel standard deviation. store those as well for retrieval by the prior
		\item only then accumulate the covariance matrix
		\item proceed with eigen-decomposition for pca whitening.
	\end{enumerate}
	\item there is also a second option, which throws away a little more information, but might be less prone to a bad set of training patches (and maybe the information thrown away is irrelevant in the first place)
	\begin{enumerate}
		\item as in 2. but instead of training set mean, take the sample mean per channel (like right now)
		\item also divide by the sample standard deviation. this step might be throwing away too much though. could also use sample mean and stored std\_dev instead
	\end{enumerate}
	\item So the necessary implementations steps look like this
	\begin{enumerate}
		\item separate out raw mat generation (channel separate even for full cov) (done)
		separate patch extraction and covariance accumulation (done)
		\item add switches (done)
		mean: sample/dataset, channel/global
		sdev: none/sample/dataset (global would just be a rescale)
		\item implement switch functionalities (partially done)
	\end{enumerate}
\end{enumerate}


\section*{August 6.}

\begin{enumerate}
	\item compare cica results to foe and ica results
	\item visualize generated conv1 feature maps for different priors
	\item try CICA conv1 priors with bigger patches
\end{enumerate}

\section*{August 5.}

\begin{enumerate}
	\item image CICA prior tests. results roughly on par with m\&v 2016, but lack behind ICA prior (6e-7 weight, no img scaling)
	\item train conv1/lin and conv2/lin CICA priors, set up single layer inversion experiments, track
\end{enumerate}

\section*{August 4.}

\begin{enumerate}
	\item half day, got CICA to finally work
\end{enumerate}


\section*{August 3.}

\begin{enumerate}
	\item set up independent ICA prior (more work than expected)
	\item visualize feature maps
\end{enumerate}



\section*{August 2.}

\begin{enumerate}
	\item set up independent ICA prior (more work than expected)
	\item set up small inversion scenario with mse loss tracking and subsequent projection to image layer
\end{enumerate}

\section*{August 1.}

\begin{enumerate}
	\item set up independent ICA prior	
	\item trained conv2lin foe prior on 6400 comps 

\end{enumerate}


\section*{July 31.}

\begin{enumerate}
	\item track ICA convergence problems
	\item tried FoE prior in comparison
\end{enumerate}

\section*{July 29.}

\begin{enumerate}
	\item started setting up channel-independent ica prior
	\item found convergence problems training large ICA priors
\end{enumerate}

\section*{July 28.}

\begin{enumerate}
	\item finished full conv2relu prior training
	\item got das4 access
\end{enumerate}

\section*{July 27.}

\begin{enumerate}
	\item meeting with jörn
	\begin{enumerate}
		\item look into distributed computation for tensorflow
		\item sign up for das4
		\item visualize learned filters
		\item consider making ica priors for independent feature maps (independence assumptions are strong, but the computations save a lot of space)
		\item consider training priors not on relu outputs but on conv output, because structure might be smoother
		\item isolate even smaller subproblem: invert single conv block (e.g. pool2 to pool1 or conv2/lin to conv1/lin). visualize reconstructed feature maps with and without prior. 
		\item set up L-BFGS as optimizer for potentially better results
	\end{enumerate}
	\item set up filter vis for feature map patch priors
	\begin{enumerate}
		\item dim reduced priors look wavelet-ish which is good but could be due to pca
		\item non-reduced priors must be done remotely. training conv2 relu prior for that
	\end{enumerate}
	\item set up up L-BFGS as pre-img optimizer (need to set up callbacks/logging) initial results don't look very good though
\end{enumerate}


\section*{July 26.}

\begin{enumerate}
	\item ran 32k prior series
	\item fixed some logging inefficiencies
\end{enumerate}


\section*{July 25.}

\begin{enumerate}
	\item half day (Bosch interview + prep)
	\item accumulated 1k prior results, experimented with img prior weighting
\end{enumerate}


\section*{July 24.}

\begin{enumerate}
	\item off day (BCCN interview, prepping Bosch interview)
\end{enumerate}

\section*{July 23.}

\begin{enumerate}
	\item experiment with combinations of conv4 mse loss, img ica prior and conv2 and conv3 ica priors
	\item different weightings of (dim reduced) conv2 prior (laptop)
	\item differnce between dim reduced and full conv3 prior (amlab pc)
\end{enumerate}

\section*{July 23.}

\begin{enumerate}
	\item off day
\end{enumerate}

\section*{July 21.}

\begin{enumerate}
	\item trained and experimented with dimensionality reduced feat map priors
\end{enumerate}


\section*{July 20.}

\begin{enumerate}
	\item some learning rate experiments with M\&V 16 code
	\item trained conv2/relu and conv3/relu priors
	\item compiled update
\end{enumerate}


\section*{July 19.}

\begin{enumerate}
	\item made patch data for pooling layer 1
	\item trained pool1 3x3 prior (with little to no discernible effect)
\end{enumerate}


\section*{July 18.}

\begin{enumerate}
	\item read invertible resnet paper
	\item revisited rim for application to net inversion
\end{enumerate}



\section*{July 17.}

\begin{enumerate}
	\item revisited clip op in mv16
	\item got mv16 to mostly work. colors in fc reconstructions still seem a bit off (less green bias)
	\item started setting up feature map patch data creation
\end{enumerate}


\section*{July 14.}

\begin{enumerate}
 	\item sent mail to Max Welling
 	\item worked on M\&V 16
\end{enumerate}

\section*{July 13.}

\begin{enumerate}
	\item read papers from jörn about quality metrics
	\item draft related works (so the big names are connected)
	\item ongoing: more ica based reconstructions
\end{enumerate}
	
\section*{July 12.}

\begin{enumerate}
	\item reworked summary:
	\subitem split methods and results
	\subitem gave some more details on result setups
	\subitem drafted motivation
\end{enumerate}

\section*{July 11.}

\begin{enumerate}
	\item refactoring, integrated M\&V into net inversion class
	\item generated first deep ica reconstruction
\end{enumerate}

\section*{July 10.}

\begin{enumerate}
	\item found out, why first ICA prior results are so bad. (sigma missing)
	\item trained first batch of foe priors (looking very similar to ica priors.)
	\item trained ica prior with more (1024) components: filters look similar. laptop gpu out of memory when computing ICA prior (but only after about 100 steps. what's going on there?)
	\item trained ica prior reconstruction with different weightings. lower weights are slower to converge, but result in crisper images.
\end{enumerate}


\section*{July 2.}

\begin{enumerate}
	\item drafted some thoughts on sampling
	\item think about weighting maybe normalize priors for same average score on natural images?
\end{enumerate}

\section*{July 1.}

\begin{enumerate}
	\item set up FoE prior
\end{enumerate}


\section*{June 30.}

\begin{enumerate}
	\item worked out score matching for field of experts 
	\item read patternnet paper	
\end{enumerate}

\section*{June 29.}

\begin{enumerate}
	\item drafted timeline
	\item drafted summary + goals from now on
\end{enumerate}

\section*{June 15.}

\subsection*{Completed}

\begin{enumerate}
    \item enable loading option for optimizer parameters (done)
    \item  add split loss option per module (done)
    \item  work through and re-implement mahendran \& vedaldi's paper (done)
    \item  test M\&V model on vgg and reproduce alexnet results (done)
    \item  adapt code to allow stacking models (done - for now)
    \item  pretty up plotting functions (done - for now)
    \item  continue training of later vgg layers to actual convergence (done - for now)
    \item  consider adding parts of curet texture data set to selected images (decided not to - for now)
    \item implement deconvs as upsampling-conv operations
    \item implement score matching ICA and overcomplete ICA models from Hyvärinen
    \item adapt code to allow arbitrary losses and priors
\end{enumerate}

\subsection*{Results}
\FloatBarrier
\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{diary_figures/mh_alexnet_overview2.png}
    \caption{Mahendran \& Vedaldi: alexnet reconstructions}
    \label{fig:mh_alex}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{diary_figures/mh_vgg16_overview2.png}
    \caption{Mahendran \& Vedaldi: vgg16 reconstructions}
    \label{fig:mh_vgg}
\end{figure}
\FloatBarrier
\subsection*{Next Steps}
\begin{enumerate}
    \item Some Deconv-Conv models need redoing (with upsample-conv-conv)
    \item MV-2016 code is not producing good results yet
    \item get Hyvärinen ICA to work
    \item expand into field of experts
    \item repeat MV work with ICA priors
    \item explore sampling options
    \item write-up in paper form
\end{enumerate}


\section*{May 10.}

\subsection*{Completed}
\begin{itemize}
    \item change architecture: each module specifies in and out tensor (optionally either from last module or classifier) and whether reconstruction becomes part of the loss. enables flexible training of multiple modules at once
    \item  create reconstructions for deeper vgg layers and alexnet
    \item enable loading weights to continue training from previous sessions
    \item create resized dataset for runtime speedups
\end{itemize}

\subsection*{Results}
Three things:
\begin{enumerate}
    \item Reconstructions in Vgg from the first pooling layer suggest that 3 stacked modules which are individually trained on each operation (conv1\_1-conv1\_2-pool1) yield better results than the same model trained end-to-end.
    \item Reconstruction quality decays gradually, as expected, when starting at deeper layers in vgg.
    \item In alexnet, the Local Response Normalization layer rapidly decreases reconstruction quality. A lot of information is lost in this step, explaining the results obtained by Dosovitskiy and Brox.
    \item A single initial test (more to come) indicates that multiple modules can be trained together, taking other reconstructions as input and optimizing the sum of the individual losses.
\end{enumerate}

\FloatBarrier

\begin{figure}
    \centering
    \includegraphics{diary_figures/alexnet_l12.png}
    \caption{AlexNet: pool1 to input. two modules trained simultaneously}
    \label{fig:may10_a_l12}
\end{figure}


\begin{figure}
    \centering
    \includegraphics{diary_figures/alexnet_l12_stacked.png}
    \caption{AlexNet: pool1 to input. two modules stacked}
    \label{fig:may10_a_l12s}
\end{figure}

\begin{figure}
    \centering
    \includegraphics{diary_figures/vgg_l123_end2end.png}
    \caption{Vgg16: pool1 to input. 3 modules trained end to end}
    \label{fig:may10_v_l123e2e}
\end{figure}

\begin{figure}
    \centering
    \includegraphics{diary_figures/vgg_l123_stacked.png}
    \caption{Vgg16: pool1 to input. 3 modules trained separately and stacked}
    \label{fig:may10_v_l123s}
\end{figure}

\begin{figure}
    \centering
    \includegraphics{diary_figures/vgg_pool2.png}
    \caption{Vgg16: pool2 to input: 6 modules stacked}
    \label{fig:may10_v_pool2}
\end{figure}

\begin{figure}
    \centering
    \includegraphics{diary_figures/vgg_pool3.png}
    \caption{Vgg16: pool3 to input: 10 modules stacked}
    \label{fig:may10_v_pool3}
\end{figure}

\FloatBarrier

\subsection*{Next Steps}
\begin{itemize}
    \item add option for loss logging per module
    \item continue training of later vgg layers to full convergence
    \item build caffe and load dosovitskiy's model weights. rebuild model in tensorflow.
    \item add curet texture database to selected images
    \item re-implement mahendran \& vedaldi
    \item build sparse coding modules
    \item plan out remainder of the project and register the thesis
\end{itemize}



\section*{May 3.}

\subsection*{Completed}
\begin{itemize}
    \item step to later layers, for simplicity with vgg first.
	\item  adapt code to allow stacking models (somewhat hacked together, needs rework later)
	\item evaluate which model (cd, dc, dd) works best on alexnet and vgg
	\item reproduce cd runs on alexnet 
	\item vary learning rate 
	\item redo vgg experiments with BGR mean order
	\item test artificial data with uniform areas to test vggnet
	\item track source of black spots, try renormalizing again
	\item rework run logging, so used parameters can be read off
	\item unify vvg and alexnet layer inversion classes
	\item track validation set loss
	\item find good way to log loss per channel
	\item pretty up plotting functions (should be sufficient for now)
\end{itemize}

\subsection*{Results}

This week has yielded two major findings:
\begin{enumerate}
    \item the first three layers of Vgg16 (two convolutions and one pooling layer) can be inverted with decent results using three stacked convolution-deconvolution models trained individually.
    \item both deconvolution-convolution and double deconvolution model (where the first operation has stride) yield fewer artifacts when inverting the first Alexnet layer (conv+relu)
\end{enumerate}

Also, the grey spots and color inaccuracies found in last weeks results were apparently owed to the model's slow learning of color biases and disappear entirely, when the normalized BGR image is used as a target.

The figures below show reconstructions of the first 3 stacked Vgg16 layers and of the first Alexnet layer using the three different models. Because differences between image and reconstruction can be quite small, two difference measures have been added. the first is a color/channel accurate measure, showing the difference between image and reconstruction, renormalized to the interval [0,1]. So if, for example, the reconstructed image has less blue in it, the measure will be blue. Areas without error (or equal errors across channels) are grey. The second difference measure is the renormalized absolute difference, which highlights areas with errors more clearly. Areas without error are black. Given renormalization,  pixel intensity only informs about relative error within the image, not absolute error compared to other images. Without normalization, the errors were hardly visible. The figures are ordered left to right as: image, reconstruction, color measure, absolute measure.

\begin{figure}
    \centering
    \includegraphics{diary_figures/stacked_inversion.png}
    \caption{reconstruction from pool1 layer by 3 stacked conv-deconv models. Left to right: Image, reconstruction, color accurate error, relative intensity accurate error}
    \label{fig:stacked_inv1}
\end{figure}

\begin{figure}
    \centering
    \includegraphics{diary_figures/stacked_selected.png}
    \caption{more reconstructions from pool1 layer by 3 stacked conv-deconv models}
    \label{fig:stacked_inv2}
\end{figure}

\begin{figure}
    \centering
    \includegraphics{diary_figures/img_vs_rec_alexnet_cd.png}
    \caption{reconstructions of the first alexnet layer using Conv-Deconv model}
    \label{fig:alex_cd}
\end{figure}


\begin{figure}
    \centering
    \includegraphics{diary_figures/img_vs_rec_alexnet_dc.png}
    \caption{reconstructions of the first alexnet layer using Deconv-Conv model}
    \label{fig:alex_dc}
\end{figure}

\begin{figure}
    \centering
    \includegraphics{diary_figures/img_vs_rec_alexnet_dd.png}
    \caption{reconstructions of the first alexnet layer using Deconv-Deconv model}
    \label{fig:alex_dd}
\end{figure}

\FloatBarrier
\subsection*{Next Steps}
\begin{itemize}
    \item invert pool1 with a deconv conv model, which deals better with strides
    \item train and stack 3 layer inversions, compare to same model trained in one go.
    \item find second source on image net means
    \item increase number of stacked layers and extend results to alexnet
    \item work out details of ICA based model
    \item specify roadmap for remaining thesis and officially register the project
\end{itemize}

\end{document}
